# Learning PyTorch along with attention-based NLP

Andrej Karpathy lecture: "Let's build GPT: from scratch, in code, spelled out." https://youtu.be/kCc8FmEb1nY
Reference repo by the lecturer: https://github.com/karpathy/nanoGPT

The code is re-worked, some comments and assorted Pytorch exercises are added along the way
This work is done for training only and the better source of the same thing is quoted above.

## m50.pt model:

- Trained using the v3.py version of the code
 - Ran for 50K steps
 - To run the m50.pt model:
```
python load.py
```

- To re-train m50.pt model:

```
python v3.py
```

- NOTE: training is deterministic (seeds are set so you'll get the same model every time)

Sample output:

```
Total of 0.46M parameters
step 0: train loss 4.6937, val loss 4.6789
step 500: train loss 2.2087, val loss 2.2329
step 1000: train loss 1.9896, val loss 2.0544
step 1500: train loss 1.8720, val loss 1.9782
...
step 48500: train loss 1.2727, val loss 1.5529
step 49000: train loss 1.2684, val loss 1.5585
step 49500: train loss 1.2696, val loss 1.5451
```

As free-fore I could shot saw to enterprehence to stay\
let him, uncle her gates? Marquish ruging such friends.

ANTIGONUS:\
May not to the where he arrive with\
a brother, that proport shall not, die; 'tis kindness\
By piece a heart.

DUKE VINCENTIO:\
So so, I see when I say,\
That through and fields and in arm spitured:\
Savants that breath the dime and present\
That those is not to the fever or honour:\
A graping of the subject may well:\
Betwixt the Volscians! Merdly me.

POMPEY:\
Nay, an't my liege in

