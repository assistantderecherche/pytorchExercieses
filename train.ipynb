{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "36f804f4",
   "metadata": {
    "id": "36f804f4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch._C import NoneType\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0631d96f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0631d96f",
    "outputId": "2130a20d-995d-47a8-e7b1-eed887a6b174"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-02-27 18:25:09--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt.2’\n",
      "\n",
      "input.txt.2         100%[===================>]   1.06M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2023-02-27 18:25:09 (8.30 MB/s) - ‘input.txt.2’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1a73ac49",
   "metadata": {
    "id": "1a73ac49"
   },
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b5a12515",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b5a12515",
    "outputId": "845a7499-3895-49f7-fae0-2942eff30c48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(len(text))\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4d9b9f49",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4d9b9f49",
    "outputId": "2df47057-01ef-4cf4-bddf-f4af9b929d57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size = 65\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"vocabulary size = {vocab_size}\")\n",
    "print(''.join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "79312641",
   "metadata": {
    "id": "79312641"
   },
   "outputs": [],
   "source": [
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] if c in stoi else -1 for c in s]\n",
    "decode = lambda l: ''.join([itos[i] if i in itos else '' for i in l ])\n",
    "tdecode = lambda l: ''.join([itos[int(i)] if int(i) in itos else '' for i in l ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9ced54e5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ced54e5",
    "outputId": "0df31da7-8d0d-4b08-bed6-5231c97c14fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43, -1, -1, -1]\n",
      " !$&'\n"
     ]
    }
   ],
   "source": [
    "print(encode(\"hii there>>>\"))\n",
    "print(decode([1,2,3,4,5, 10000, 10000000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "256de0e2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "256de0e2",
    "outputId": "a2814700-a2aa-47e9-91df-eb806e5f8a71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hii there\n"
     ]
    }
   ],
   "source": [
    "print(decode(encode('hii there>>>')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a765985e",
   "metadata": {
    "id": "a765985e"
   },
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1856d18d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1856d18d",
    "outputId": "dc4b596a-2110-4c35-e09a-c1c743dad560"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394])\n",
      "torch.LongTensor\n",
      "1115394\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(data.type())\n",
    "print(len(text))\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cf824443",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cf824443",
    "outputId": "a86f07a0-bdbc-4bc2-f710-7619d12d74b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citi\n",
      "First Citi\n"
     ]
    }
   ],
   "source": [
    "print(decode([int(x) for x in data[0:10]]))\n",
    "print(tdecode(data[0:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "dcb38abf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dcb38abf",
    "outputId": "499c6357-1e60-4af3-ce19-f2daae070c69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "n = int(0.9*len(data))\n",
    "train_data=data[:n]\n",
    "val_data=data[n:]\n",
    "\n",
    "all = torch.cat((train_data,val_data))\n",
    "print(len(all)-len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f81799ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f81799ca",
    "outputId": "2b62b51e-ce64-4f6a-c028-a7982a90bc8b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "batch_size = 4\n",
    "train_data[1:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "GxLXqcHaxaZq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GxLXqcHaxaZq",
    "outputId": "7d10899c-918a-465b-b226-a958aed7ccc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "x = tensor([18, 47, 56, 57, 58,  1, 15, 47])\n",
      "y   =   tensor([47, 56, 57, 58,  1, 15, 47, 58])\n",
      "\n",
      "\n",
      "context = tensor([18]), target = 47\n",
      "context = tensor([18, 47]), target = 56\n",
      "context = tensor([18, 47, 56]), target = 57\n",
      "context = tensor([18, 47, 56, 57]), target = 58\n",
      "context = tensor([18, 47, 56, 57, 58]), target = 1\n",
      "context = tensor([18, 47, 56, 57, 58,  1]), target = 15\n",
      "context = tensor([18, 47, 56, 57, 58,  1, 15]), target = 47\n",
      "context = tensor([18, 47, 56, 57, 58,  1, 15, 47]), target = 58\n"
     ]
    }
   ],
   "source": [
    "x=train_data[:block_size]\n",
    "print(f\"\\n\\nx = {x}\")\n",
    "y=train_data[1:block_size+1]\n",
    "print(f\"y   =   {y}\\n\\n\")\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"context = {context}, target = {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "yHyLkyNWO_JS",
   "metadata": {
    "id": "yHyLkyNWO_JS"
   },
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split =='train' else val_data\n",
    "    ix = torch.randint(len(data)-block_size, (batch_size,))\n",
    "    x = torch.stack([data[t:t+block_size] for t in ix])\n",
    "    y = torch.stack([data[t+1: t+block_size+1] for t in ix])\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "pQJX7-ad15IB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pQJX7-ad15IB",
    "outputId": "ea0536cc-ddc8-4d18-f86c-c7dfc9747ea6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "torch.Size([4, 8]) torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "xb, yb = get_batch('train')\n",
    "print(xb)\n",
    "print(yb)\n",
    "print(xb.shape, yb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6EYlZ_oXSakZ",
   "metadata": {
    "id": "6EYlZ_oXSakZ"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # how the 'Bigram' part is related to the embedding dimensionality?\n",
    "        # both num_embeddings and embedding_dim equal to vocab_size (!!!) \n",
    "        # manual: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is the same as x: sequences of len block_size and targets are the\n",
    "        # same as before (y), the correct expected outcomes\n",
    "        # -- also looked up in the embedding table?\n",
    "\n",
    "        logits = self.token_embedding_table(idx) #(B,T,C) batch, time, channel\n",
    "        # channel is the vocabulary size (!!!); as if number of colurs = 65 :)\n",
    "        #print(f\"logits type: {type(logits)}, logits shape: {logits.shape}\")\n",
    "    \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "\n",
    "            #\n",
    "            # Strangely enough cross_entropy takes logits as \"weights\" and targets as \"class labels\"\n",
    "            #\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "      \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        print(f\"==> generate, idx.shape = {idx.shape}\")\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx);\n",
    "            logits = logits[:,-1,:] # the last time slice, (BxC)\n",
    "\n",
    "              # for each batch B get a distribution of classes C\n",
    "            probs = F.softmax(logits, dim=-1) \n",
    "\n",
    "              # 1 sample from each row for each batch\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) \n",
    "            idx = torch.cat((idx, idx_next), dim = 1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "WErYyDAmvBHA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WErYyDAmvBHA",
    "outputId": "1f7967c3-4f79-4940-8d7d-617a4bf69691"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xb.shape = torch.Size([4, 8])\n",
      "yb.shape = torch.Size([4, 8])\n",
      "--> logits.shape = torch.Size([32, 65])\n",
      "--> this is because batch_size*block_size = 32\n",
      "\n",
      "logits at index = 12:\n",
      "[ 0.2474621  -0.63485116 -1.2909483   1.1821823   0.14786936 -0.43331397\n",
      " -0.8269277   0.07280172 -1.2982308   0.39599574 -1.2460201   0.14583187\n",
      " -0.5699396  -1.3560567  -0.38120747 -0.8514603   1.1917949  -0.81080186\n",
      " -0.17326038 -0.47029358 -0.60004216 -1.3636268  -1.0889153   1.0108203\n",
      "  0.85429174 -0.04411305  1.8016624   0.60141    -2.5448313  -0.48651642\n",
      "  2.6412039   1.6052898   0.59007245  0.81368    -0.11238304 -0.30501363\n",
      "  1.1426241   0.66372484 -0.7000075   0.9262019  -1.103203   -1.2124757\n",
      "  0.6065394   0.5881612  -0.5452641   0.7654137   0.5691515   0.8859054\n",
      " -0.07004447  0.67918706 -0.02830357 -1.22435    -1.7192171   1.4801265\n",
      "  0.9586657  -0.03378088  0.5083099  -0.2501664   2.0734181  -0.29940873\n",
      "  0.04729307 -0.9625754   1.3064294  -0.22557093 -1.8304833 ]\n",
      "\n",
      "offset = 30, value = 2.6412038803100586\n",
      "\n",
      "?  -->  R \n",
      "\n",
      "loss = 4.878634929656982, vs expected(?) 4.174387454986572\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "print(f\"xb.shape = {xb.shape}\")\n",
    "print(f\"yb.shape = {yb.shape}\")\n",
    "\n",
    "\n",
    "logits, loss = m(xb,yb)\n",
    "print(f\"--> logits.shape = {logits.shape}\")\n",
    "print(f\"--> this is because batch_size*block_size = {batch_size*block_size}\\n\")\n",
    "ind = 12\n",
    "print(f\"logits at index = {ind}:\\n{logits[ind,:].detach().numpy()}\\n\")\n",
    "offset_argmax = torch.argmax(logits[ind,:])\n",
    "val = logits[ind, offset_argmax]\n",
    "print(f\"offset = {offset_argmax}, value = {val}\\n\")\n",
    "print(decode([ind]), ' --> ', decode([int(offset_argmax)]),'\\n')\n",
    "print(f\"loss = {loss}, vs expected(?) {float(-torch.log(torch.Tensor([1/vocab_size])))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "eQNXBVB8WbGW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eQNXBVB8WbGW",
    "outputId": "7beaa97b-0aeb-49f3-d820-0dcc4315debd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> generate, idx.shape = torch.Size([1, 1])\n",
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "print(decode(m.generate(idx, max_new_tokens = 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "A__bxLP3rlPU",
   "metadata": {
    "id": "A__bxLP3rlPU"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "PZle72Oi_tYT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PZle72Oi_tYT",
    "outputId": "e21d9776-0963-4768-a972-cea701570fc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count = 0:\n",
      "Parameter containing:\n",
      "tensor([[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
      "        [ 0.5978, -0.0514, -0.0646,  ..., -1.4649, -2.0555,  1.8275],\n",
      "        [ 1.3035, -0.4501,  1.3471,  ...,  0.1910, -0.3425,  1.7955],\n",
      "        ...,\n",
      "        [ 0.4222, -1.8111, -1.0118,  ...,  0.5462,  0.2788,  0.7280],\n",
      "        [-0.8109,  0.2410, -0.1139,  ...,  1.4509,  0.1836,  0.3064],\n",
      "        [-1.4322, -0.2810, -2.2789,  ..., -0.5551,  1.0666,  0.5364]],\n",
      "       requires_grad=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# discussion about how to add custom parameters to Pytorch\n",
    "# https://stackoverflow.com/questions/59234238/how-to-add-parameters-in-module-class-in-pytorch-custom-model\n",
    "# you need to \n",
    "# --> 'register' the parameter:\n",
    "#\n",
    "# self.register_parameter(name='bias', param = torch.nn.Parameter(torch.randn(3)))\n",
    "# \n",
    "# NOT SIMPLY:\n",
    "#\n",
    "# self.bias = torch.nn.Parameter(torch.rand())\n",
    "#\n",
    "# --> this is strange: why would call torch.nn.Parameter but not register it at the same time????\n",
    "#\n",
    "\n",
    "for ind,x in enumerate(m.parameters()):\n",
    "    print(f\"count = {ind}:\\n{x}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "dZSRn1mhBbW1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dZSRn1mhBbW1",
    "outputId": "cf0a87e1-7066-43a8-b9e8-d3efcc876db6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.692410945892334\n",
      "100 4.621085166931152\n",
      "200 4.549462795257568\n",
      "300 4.345611572265625\n",
      "400 4.25573205947876\n",
      "500 4.214480876922607\n",
      "600 4.124096870422363\n",
      "700 3.9863951206207275\n",
      "800 3.9517807960510254\n",
      "900 3.837888717651367\n"
     ]
    }
   ],
   "source": [
    "batch_size=32\n",
    "\n",
    "for steps in range(1000):\n",
    "    xb,yb = get_batch('train')\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if steps % 100 == 0:\n",
    "        print(steps, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "GTeLUX4XD617",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GTeLUX4XD617",
    "outputId": "c27a38b1-1c56-48f4-90f4-60e98adb3d6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> generate, idx.shape = torch.Size([1, 1])\n",
      "\n",
      "Wh;;Sq.f ustNzknc\n",
      "kwgOj$dhPWr,SV?hsusiKpgXXUh;Apmem d?hESXI.i;TrJgkiF-oKbXCAA -botrngFCHAUQkn$\n",
      "\n",
      "pn$w-gHoi?wtd!\n",
      "LLULIfSK'bAw :M.ZtOptXEQcL?hfaofqbPd?OnonQQJMap$aypupIBYGUsZaI'ottllo..k$W$Akp?yl?ajKlzY!lx&QQLW? t,bXFkyhl-dmVsHeckhRl,jSClgjuk:3Iv\n",
      "?OqlrV;!Plxfzgy;;\n",
      "'mRjuBQ&xk!$\n",
      "h\n",
      "SiruDJgKuDny,S$ERf.?GSV-ivvKcOvi-nQGX&q-YQbm dEM?px;Akr-IESq--wIWId\n",
      "RFgXTpDUgM:CK$I!uo'IBT -\n",
      "j?wfy fFr.&fiqtRS.ZttxGh' a!ogrn$zoZqbocL&yIffBDWNUboscuQqo.Fls,?,M?eZxHx?p?EV.mJiHqHnxT  bQpa;P fawiF$-QbWv&f:CVDCBfano,b?$Esev.?\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "print(decode(m.generate(idx, max_new_tokens = 500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "cac07e06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0a40fd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]], dtype=torch.float64)\n",
      "tensor([[[58., 16.],\n",
      "         [55., 92.],\n",
      "         [14., 74.]],\n",
      "\n",
      "        [[47.,  4.],\n",
      "         [25., 60.],\n",
      "         [55., 23.]],\n",
      "\n",
      "        [[68., 79.],\n",
      "         [42., 17.],\n",
      "         [53., 49.]],\n",
      "\n",
      "        [[44., 61.],\n",
      "         [26., 28.],\n",
      "         [36., 69.]]], dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[58.0000, 16.0000],\n",
       "         [56.5000, 54.0000],\n",
       "         [42.3333, 60.6667]],\n",
       "\n",
       "        [[47.0000,  4.0000],\n",
       "         [36.0000, 32.0000],\n",
       "         [42.3333, 29.0000]],\n",
       "\n",
       "        [[68.0000, 79.0000],\n",
       "         [55.0000, 48.0000],\n",
       "         [54.3333, 48.3333]],\n",
       "\n",
       "        [[44.0000, 61.0000],\n",
       "         [35.0000, 44.5000],\n",
       "         [35.3333, 52.6667]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T=3\n",
    "wei = torch.tril(torch.ones(T,T))\n",
    "wei = wei / torch.sum(wei,1, keepdim=True).to(torch.float64)\n",
    "print(wei)\n",
    "xx = torch.randint(100, (B,T,C)).to(torch.float64)\n",
    "print(xx)\n",
    "wei @ xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "5d24a2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xb.shape = torch.Size([32, 8])\n",
      "yb.shape = torch.Size([32, 8])\n",
      "torch.Size([65])\n",
      "torch.Size([2, 65])\n",
      "torch.Size([5, 65])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# If given a range of indices of length K as input, \n",
    "# Embedding returns a 2d, [K,vocab_size] tensor\n",
    "#\n",
    "torch.manual_seed(1337)\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "print(f\"xb.shape = {xb.shape}\")\n",
    "print(f\"yb.shape = {yb.shape}\")\n",
    "\n",
    "_,_ = m(xb,yb)\n",
    "\n",
    "t0=m.token_embedding_table(torch.tensor(0))\n",
    "print(t0.shape)\n",
    "t00=m.token_embedding_table(torch.tensor([0,0]))\n",
    "print(t00.shape)\n",
    "t5=m.token_embedding_table(torch.arange(5))\n",
    "print(t5.shape)\n",
    "\n",
    "torch.allclose(t00[0,:], t00[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d3f2a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
