{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36f804f4",
   "metadata": {
    "id": "36f804f4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch._C import NoneType\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0631d96f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0631d96f",
    "outputId": "2130a20d-995d-47a8-e7b1-eed887a6b174"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "# If the names have not been downloaded\n",
    "if not os.path.exists('input.txt'):\n",
    "    # download the names.txt file from github\n",
    "    !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "        \n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "print(len(text))\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d9b9f49",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4d9b9f49",
    "outputId": "2df47057-01ef-4cf4-bddf-f4af9b929d57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size = 65\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"vocabulary size = {vocab_size}\")\n",
    "print(''.join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79312641",
   "metadata": {
    "id": "79312641"
   },
   "outputs": [],
   "source": [
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] if c in stoi else -1 for c in s]\n",
    "decode = lambda l: ''.join([itos[i] if i in itos else '' for i in l ])\n",
    "tdecode = lambda l: ''.join([itos[int(i)] if int(i) in itos else '' for i in l ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ced54e5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ced54e5",
    "outputId": "0df31da7-8d0d-4b08-bed6-5231c97c14fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43, -1, -1, -1]\n",
      " !$&'\n"
     ]
    }
   ],
   "source": [
    "print(encode(\"hii there>>>\"))\n",
    "print(decode([1,2,3,4,5, 10000, 10000000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "256de0e2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "256de0e2",
    "outputId": "a2814700-a2aa-47e9-91df-eb806e5f8a71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hii there\n"
     ]
    }
   ],
   "source": [
    "print(decode(encode('hii there>>>')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a765985e",
   "metadata": {
    "id": "a765985e"
   },
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1856d18d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1856d18d",
    "outputId": "dc4b596a-2110-4c35-e09a-c1c743dad560"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394])\n",
      "torch.LongTensor\n",
      "1115394\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(data.type())\n",
    "print(len(text))\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf824443",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cf824443",
    "outputId": "a86f07a0-bdbc-4bc2-f710-7619d12d74b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citi\n",
      "First Citi\n"
     ]
    }
   ],
   "source": [
    "print(decode([int(x) for x in data[0:10]]))\n",
    "print(tdecode(data[0:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcb38abf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dcb38abf",
    "outputId": "499c6357-1e60-4af3-ce19-f2daae070c69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "n = int(0.9*len(data))\n",
    "train_data=data[:n]\n",
    "val_data=data[n:]\n",
    "\n",
    "all = torch.cat((train_data,val_data))\n",
    "print(len(all)-len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f81799ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f81799ca",
    "outputId": "2b62b51e-ce64-4f6a-c028-a7982a90bc8b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "batch_size = 4\n",
    "train_data[1:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "GxLXqcHaxaZq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GxLXqcHaxaZq",
    "outputId": "7d10899c-918a-465b-b226-a958aed7ccc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "x = tensor([18, 47, 56, 57, 58,  1, 15, 47])\n",
      "y   =   tensor([47, 56, 57, 58,  1, 15, 47, 58])\n",
      "\n",
      "\n",
      "context = tensor([18]), target = 47\n",
      "context = tensor([18, 47]), target = 56\n",
      "context = tensor([18, 47, 56]), target = 57\n",
      "context = tensor([18, 47, 56, 57]), target = 58\n",
      "context = tensor([18, 47, 56, 57, 58]), target = 1\n",
      "context = tensor([18, 47, 56, 57, 58,  1]), target = 15\n",
      "context = tensor([18, 47, 56, 57, 58,  1, 15]), target = 47\n",
      "context = tensor([18, 47, 56, 57, 58,  1, 15, 47]), target = 58\n"
     ]
    }
   ],
   "source": [
    "x=train_data[:block_size]\n",
    "print(f\"\\n\\nx = {x}\")\n",
    "y=train_data[1:block_size+1]\n",
    "print(f\"y   =   {y}\\n\\n\")\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"context = {context}, target = {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "yHyLkyNWO_JS",
   "metadata": {
    "id": "yHyLkyNWO_JS"
   },
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split =='train' else val_data\n",
    "    ix = torch.randint(len(data)-block_size, (batch_size,))\n",
    "    x = torch.stack([data[t:t+block_size] for t in ix])\n",
    "    y = torch.stack([data[t+1: t+block_size+1] for t in ix])\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "pQJX7-ad15IB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pQJX7-ad15IB",
    "outputId": "ea0536cc-ddc8-4d18-f86c-c7dfc9747ea6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "torch.Size([4, 8]) torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "xb, yb = get_batch('train')\n",
    "print(xb)\n",
    "print(yb)\n",
    "print(xb.shape, yb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6EYlZ_oXSakZ",
   "metadata": {
    "id": "6EYlZ_oXSakZ"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # how the 'Bigram' part is related to the embedding dimensionality?\n",
    "        # both num_embeddings and embedding_dim equal to vocab_size (!!!) \n",
    "        # manual: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is the same as x: sequences of len block_size and targets are the\n",
    "        # same as before (y), the correct expected outcomes\n",
    "        # -- also looked up in the embedding table?\n",
    "\n",
    "        logits = self.token_embedding_table(idx) #(B,T,C) batch, time, channel\n",
    "        # channel is the vocabulary size (!!!); as if number of colurs = 65 :)\n",
    "        #print(f\"logits type: {type(logits)}, logits shape: {logits.shape}\")\n",
    "    \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "\n",
    "            #\n",
    "            # Strangely enough cross_entropy takes logits as \"weights\" and targets as \"class labels\"\n",
    "            #\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "      \n",
    "        return logits, loss\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "#     def generate(self, idx, max_new_tokens):\n",
    "#         print(f\"==> generate, idx.shape = {idx.shape}\")\n",
    "#         for _ in range(max_new_tokens):\n",
    "#             logits, loss = self(idx);\n",
    "#             logits = logits[:,-1,:] # the last time slice, (BxC)\n",
    "\n",
    "#               # for each batch B get a distribution of classes C\n",
    "#             probs = F.softmax(logits, dim=-1) \n",
    "\n",
    "#               # 1 sample from each row for each batch\n",
    "#             idx_next = torch.multinomial(probs, num_samples=1) \n",
    "#             idx = torch.cat((idx, idx_next), dim = 1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "WErYyDAmvBHA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WErYyDAmvBHA",
    "outputId": "1f7967c3-4f79-4940-8d7d-617a4bf69691"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xb.shape = torch.Size([4, 8])\n",
      "yb.shape = torch.Size([4, 8])\n",
      "--> logits.shape = torch.Size([32, 65])\n",
      "--> this is because batch_size*block_size = 32\n",
      "\n",
      "logits at index = 12:\n",
      "[ 0.2474621  -0.63485116 -1.2909483   1.1821823   0.14786936 -0.43331397\n",
      " -0.8269277   0.07280172 -1.2982308   0.39599574 -1.2460201   0.14583187\n",
      " -0.5699396  -1.3560567  -0.38120747 -0.8514603   1.1917949  -0.81080186\n",
      " -0.17326038 -0.47029358 -0.60004216 -1.3636268  -1.0889153   1.0108203\n",
      "  0.85429174 -0.04411305  1.8016624   0.60141    -2.5448313  -0.48651642\n",
      "  2.6412039   1.6052898   0.59007245  0.81368    -0.11238304 -0.30501363\n",
      "  1.1426241   0.66372484 -0.7000075   0.9262019  -1.103203   -1.2124757\n",
      "  0.6065394   0.5881612  -0.5452641   0.7654137   0.5691515   0.8859054\n",
      " -0.07004447  0.67918706 -0.02830357 -1.22435    -1.7192171   1.4801265\n",
      "  0.9586657  -0.03378088  0.5083099  -0.2501664   2.0734181  -0.29940873\n",
      "  0.04729307 -0.9625754   1.3064294  -0.22557093 -1.8304833 ]\n",
      "\n",
      "offset = 30, value = 2.6412038803100586\n",
      "\n",
      "?  -->  R \n",
      "\n",
      "loss = 4.878634929656982, vs expected(?) 4.174387454986572\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "print(f\"xb.shape = {xb.shape}\")\n",
    "print(f\"yb.shape = {yb.shape}\")\n",
    "\n",
    "\n",
    "logits, loss = m(xb,yb)\n",
    "print(f\"--> logits.shape = {logits.shape}\")\n",
    "print(f\"--> this is because batch_size*block_size = {batch_size*block_size}\\n\")\n",
    "ind = 12\n",
    "print(f\"logits at index = {ind}:\\n{logits[ind,:].detach().numpy()}\\n\")\n",
    "offset_argmax = torch.argmax(logits[ind,:])\n",
    "val = logits[ind, offset_argmax]\n",
    "print(f\"offset = {offset_argmax}, value = {val}\\n\")\n",
    "print(decode([ind]), ' --> ', decode([int(offset_argmax)]),'\\n')\n",
    "print(f\"loss = {loss}, vs expected(?) {float(-torch.log(torch.Tensor([1/vocab_size])))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eQNXBVB8WbGW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eQNXBVB8WbGW",
    "outputId": "7beaa97b-0aeb-49f3-d820-0dcc4315debd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> generate, idx.shape = torch.Size([1, 1])\n",
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "print(decode(m.generate(idx, max_new_tokens = 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "A__bxLP3rlPU",
   "metadata": {
    "id": "A__bxLP3rlPU"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "PZle72Oi_tYT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PZle72Oi_tYT",
    "outputId": "e21d9776-0963-4768-a972-cea701570fc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count = 0:\n",
      "Parameter containing:\n",
      "tensor([[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
      "        [ 0.5978, -0.0514, -0.0646,  ..., -1.4649, -2.0555,  1.8275],\n",
      "        [ 1.3035, -0.4501,  1.3471,  ...,  0.1910, -0.3425,  1.7955],\n",
      "        ...,\n",
      "        [ 0.4222, -1.8111, -1.0118,  ...,  0.5462,  0.2788,  0.7280],\n",
      "        [-0.8109,  0.2410, -0.1139,  ...,  1.4509,  0.1836,  0.3064],\n",
      "        [-1.4322, -0.2810, -2.2789,  ..., -0.5551,  1.0666,  0.5364]],\n",
      "       requires_grad=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# discussion about how to add custom parameters to Pytorch\n",
    "# https://stackoverflow.com/questions/59234238/how-to-add-parameters-in-module-class-in-pytorch-custom-model\n",
    "# you need to \n",
    "# --> 'register' the parameter:\n",
    "#\n",
    "# self.register_parameter(name='bias', param = torch.nn.Parameter(torch.randn(3)))\n",
    "# \n",
    "# NOT SIMPLY:\n",
    "#\n",
    "# self.bias = torch.nn.Parameter(torch.rand())\n",
    "#\n",
    "# --> this is strange: why would call torch.nn.Parameter but not register it at the same time????\n",
    "#\n",
    "\n",
    "for ind,x in enumerate(m.parameters()):\n",
    "    print(f\"count = {ind}:\\n{x}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dZSRn1mhBbW1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dZSRn1mhBbW1",
    "outputId": "cf0a87e1-7066-43a8-b9e8-d3efcc876db6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.692410945892334\n",
      "100 4.621085166931152\n",
      "200 4.549462795257568\n",
      "300 4.345611572265625\n",
      "400 4.25573205947876\n",
      "500 4.214480876922607\n",
      "600 4.124096870422363\n",
      "700 3.9863951206207275\n",
      "800 3.9517807960510254\n",
      "900 3.837888717651367\n"
     ]
    }
   ],
   "source": [
    "batch_size=32\n",
    "\n",
    "for steps in range(1000):\n",
    "    xb,yb = get_batch('train')\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if steps % 100 == 0:\n",
    "        print(steps, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "GTeLUX4XD617",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GTeLUX4XD617",
    "outputId": "c27a38b1-1c56-48f4-90f4-60e98adb3d6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> generate, idx.shape = torch.Size([1, 1])\n",
      "\n",
      "Wh;;Sq.f ustNzknc\n",
      "kwgOj$dhPWr,SV?hsusiKpgXXUh;Apmem d?hESXI.i;TrJgkiF-oKbXCAA -botrngFCHAUQkn$\n",
      "\n",
      "pn$w-gHoi?wtd!\n",
      "LLULIfSK'bAw :M.ZtOptXEQcL?hfaofqbPd?OnonQQJMap$aypupIBYGUsZaI'ottllo..k$W$Akp?yl?ajKlzY!lx&QQLW? t,bXFkyhl-dmVsHeckhRl,jSClgjuk:3Iv\n",
      "?OqlrV;!Plxfzgy;;\n",
      "'mRjuBQ&xk!$\n",
      "h\n",
      "SiruDJgKuDny,S$ERf.?GSV-ivvKcOvi-nQGX&q-YQbm dEM?px;Akr-IESq--wIWId\n",
      "RFgXTpDUgM:CK$I!uo'IBT -\n",
      "j?wfy fFr.&fiqtRS.ZttxGh' a!ogrn$zoZqbocL&yIffBDWNUboscuQqo.Fls,?,M?eZxHx?p?EV.mJiHqHnxT  bQpa;P fawiF$-QbWv&f:CVDCBfano,b?$Esev.?\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "print(decode(m.generate(idx, max_new_tokens = 500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fcb180fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "#\n",
    "#  randn torch.creates a sample of N(0,1)\n",
    "#  I imagine a sample of size B*T*C is just reshaped, what else?\n",
    "#\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "582525c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]], dtype=torch.float64)\n",
      "tensor([[[58., 16.],\n",
      "         [55., 92.],\n",
      "         [14., 74.]],\n",
      "\n",
      "        [[47.,  4.],\n",
      "         [25., 60.],\n",
      "         [55., 23.]],\n",
      "\n",
      "        [[68., 79.],\n",
      "         [42., 17.],\n",
      "         [53., 49.]],\n",
      "\n",
      "        [[44., 61.],\n",
      "         [26., 28.],\n",
      "         [36., 69.]]], dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[58.0000, 16.0000],\n",
       "         [56.5000, 54.0000],\n",
       "         [42.3333, 60.6667]],\n",
       "\n",
       "        [[47.0000,  4.0000],\n",
       "         [36.0000, 32.0000],\n",
       "         [42.3333, 29.0000]],\n",
       "\n",
       "        [[68.0000, 79.0000],\n",
       "         [55.0000, 48.0000],\n",
       "         [54.3333, 48.3333]],\n",
       "\n",
       "        [[44.0000, 61.0000],\n",
       "         [35.0000, 44.5000],\n",
       "         [35.3333, 52.6667]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T=3\n",
    "wei = torch.tril(torch.ones(T,T))\n",
    "wei = wei / torch.sum(wei,1, keepdim=True).to(torch.float64)\n",
    "print(wei)\n",
    "xx = torch.randint(100, (B,T,C)).to(torch.float64)\n",
    "print(xx)\n",
    "wei @ xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "105f4d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xb.shape = torch.Size([32, 8])\n",
      "yb.shape = torch.Size([32, 8])\n",
      "torch.Size([65])\n",
      "torch.Size([2, 65])\n",
      "torch.Size([5, 65])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# If given a range of indices of length K as input, \n",
    "# Embedding returns a 2d, [K,vocab_size] tensor\n",
    "#\n",
    "torch.manual_seed(1337)\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "print(f\"xb.shape = {xb.shape}\")\n",
    "print(f\"yb.shape = {yb.shape}\")\n",
    "\n",
    "_,_ = m(xb,yb)\n",
    "\n",
    "t0=m.token_embedding_table(torch.tensor(0))\n",
    "print(t0.shape)\n",
    "t00=m.token_embedding_table(torch.tensor([0,0]))\n",
    "print(t00.shape)\n",
    "t5=m.token_embedding_table(torch.arange(5))\n",
    "print(t5.shape)\n",
    "\n",
    "torch.allclose(t00[0,:], t00[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d976f077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 16]) torch.Size([4, 8, 16]) torch.Size([4, 8, 16]) torch.Size([4, 8, 8])\n",
      "\n",
      "No longer we have: torch.Size([4, 8, 32])\n",
      "But instead: torch.Size([4, 8, 16])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 \n",
    "x=torch.randn(B,T,C)\n",
    "\n",
    "head_size=16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "\n",
    "k = key(x)\n",
    "q = query(x)\n",
    "v = value(x)\n",
    "\n",
    "wei = q @ k.transpose(-1,-2)\n",
    "\n",
    "print(k.shape, q.shape, v.shape, wei.shape)\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "#\n",
    "# this is no more:\n",
    "#wei = torch.zeros(T,T)\n",
    "#\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=1)\n",
    "#\n",
    "#  NOTE that this is the 'decoder architecture'\n",
    "#       to get the 'encoder architecture' just relax\n",
    "#       the 'do not look in the future' constraint (no torch.tril), \n",
    "#       but just wei=F.softmax without the '-inf' inserted in the future\n",
    "#       \n",
    "#\n",
    "\n",
    "#\n",
    "# this is no more:\n",
    "out = wei @ x\n",
    "print(f\"\\nNo longer we have: {out.shape}\")\n",
    "#\n",
    "# instead:\n",
    "\n",
    "out = wei @ v\n",
    "print(f\"But instead: {out.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d4b3a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0248, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0052, 0.0091, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0521, 0.0135, 0.2482, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3171, 0.0214, 0.1642, 0.1188, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0412, 0.0487, 0.1046, 0.0742, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1060, 0.5347, 0.2059, 0.1030, 0.7402, 0.0192, 0.0000, 0.0000],\n",
       "        [0.4298, 0.3409, 0.1769, 0.2027, 0.0480, 0.8472, 0.2329, 0.0000],\n",
       "        [0.0238, 0.0316, 0.1002, 0.5013, 0.0117, 0.1336, 0.7671, 1.0000]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# non-zero weights are allowed to be different:\n",
    "#\n",
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d39ddfa",
   "metadata": {},
   "source": [
    "# Discussion of cross attention\n",
    "\n",
    "querie (can) come from one source and keys and values from another. \n",
    "It's a natural separation if viewed from the standpoint of information retrieval: \n",
    "       \n",
    "- keys are how data being 'retrieved' can be identified (found)      \n",
    "- values are actual data retrieved\n",
    "\n",
    "On the other hand \n",
    "\n",
    "- queries (can) come from a different source (search string in a search engine for example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3515ef10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "without normalization:\n",
      "\n",
      "q.var() = 0.33860543370246887\n",
      "wei.var() = 1.9223554134368896\n",
      "q.var()/wei.var() = 0.1761409044265747\n",
      "\n",
      "after normalization:\n",
      "\n",
      "q.var() = 0.33860543370246887\n",
      "wei.var() = 0.1201472133398056\n",
      "q.var()/wei.var() = 2.8182544708251953\n"
     ]
    }
   ],
   "source": [
    "# Scaled attention \n",
    "\n",
    "#\n",
    "# Without normalization:\n",
    "#\n",
    "print('\\nwithout normalization:\\n')\n",
    "wei = q @ k.transpose(-1,-2)\n",
    "print(f\"q.var() = {q.var()}\")\n",
    "print(f\"wei.var() = {wei.var()}\")\n",
    "print(f\"q.var()/wei.var() = {q.var()/wei.var()}\")\n",
    "\n",
    "# Normalize by the head_size: \n",
    "wei = q @ k.transpose(-1,-2) * head_size**-0.5\n",
    "print('\\nafter normalization:\\n')\n",
    "print(f\"q.var() = {q.var()}\")\n",
    "print(f\"wei.var() = {wei.var()}\")\n",
    "print(f\"q.var()/wei.var() = {q.var()/wei.var()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29da28cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n",
      "tensor([4.2484e-18, 3.9754e-31, 2.0612e-09, 3.9754e-31, 1.0000e+00])\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# keeping wei relatively \"diffuse\"\n",
    "# lest softmax --> 1-hot\n",
    "#\n",
    "\n",
    "# for example\n",
    "\n",
    "x = torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])\n",
    "print(torch.softmax(x, dim=-1))\n",
    "print(torch.softmax(100*x, dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "99e902b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.742486476898193\n",
      "4.780079364776611\n",
      "4.6079840660095215\n",
      "4.688343524932861\n",
      "4.654579162597656\n",
      "4.767638206481934\n",
      "4.644798755645752\n",
      "4.694546222686768\n",
      "4.6973443031311035\n",
      "4.674440383911133\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(111)\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "opt = torch.optim.SGD(m.parameters(), lr=1e-4, momentum=0.9)\n",
    "opt_start_state = opt.state_dict()\n",
    "itt = 10\n",
    "\n",
    "for _ in range(itt):\n",
    "    xb, yb = get_batch('train')\n",
    "    _, loss = m(xb,yb)\n",
    "    print(loss.item())\n",
    "    # why zero_grad each iteration?\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "opt_end_state = opt.state_dict()\n",
    "\n",
    "opt_start_state == opt_end_state\n",
    "\n",
    "torch.save(m, 'sgd.pt')\n",
    "\n",
    "mm = torch.load('sgd.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f501096a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "r'nw!Ryt,Q?K-!J:'XyHE?,AlkpOymmmzF-cvS:mcvK,DaJ.'DIIfui.F\n",
      "R;kNygBhuHF;TXFtcMrAj\n",
      "!T&FXNSzFv&-Cgz,Xglw\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "print(decode(mm.generate(idx, max_new_tokens = 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2978e396",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'Block' on <module '__main__'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m m50K \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mm50K.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(decode(m50K\u001b[38;5;241m.\u001b[39mgenerate(idx, max_new_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()))\n",
      "File \u001b[0;32m/venv/torch/lib64/python3.9/site-packages/torch/serialization.py:789\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    787\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    788\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 789\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[1;32m    791\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/venv/torch/lib64/python3.9/site-packages/torch/serialization.py:1131\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1130\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1131\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/venv/torch/lib64/python3.9/site-packages/torch/serialization.py:1124\u001b[0m, in \u001b[0;36m_load.<locals>.UnpicklerWrapper.find_class\u001b[0;34m(self, mod_name, name)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m mod_name \u001b[38;5;241m=\u001b[39m load_module_mapping\u001b[38;5;241m.\u001b[39mget(mod_name, mod_name)\n\u001b[0;32m-> 1124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't get attribute 'Block' on <module '__main__'>"
     ]
    }
   ],
   "source": [
    "m50K = torch.load('m50K.pt')\n",
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "print(decode(m50K.generate(idx, max_new_tokens = 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b2242717",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'm50K' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mdir\u001b[39m(\u001b[43mm50K\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'm50K' is not defined"
     ]
    }
   ],
   "source": [
    "dir(m50K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdfde37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
